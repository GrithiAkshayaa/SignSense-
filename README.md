**_SIGNSENSE_**
SignSense is a pioneering system developed to facilitate communication between deaf or hard-of-hearing individuals and the hearing community. This integrated solution combines sign language recognition, text translation, and voice synthesis, utilizing Keras for model development and Pyttsx3 for spoken output.

The system operates in two stages: First, a convolutional neural network (CNN) accurately identifies and interprets hand gestures from a dataset of sign language gestures. Using Keras, this CNN model achieves high precision in real-time, recognizing signs and translating them into coherent text through sequence-to-sequence models or similar techniques. In the second stage, the translated text is converted into voice using Pyttsx3, making communication seamless and accessible.

Experimental results highlight SignSenseâ€™s effectiveness in real-time sign language recognition, accurate text translation, and natural voice output, making it a valuable tool for accessible communication. SignSense aims to support more inclusive communication across social, educational, and professional environments.

**SYSTEM ARCHITECTURE**
![image](https://github.com/user-attachments/assets/b8f8eb9b-a38f-4cc5-b004-0445ecaa4d51)

**The Final Working of the Sign Language Recognition and Translation is attached below.**
![image](https://github.com/user-attachments/assets/48d953f2-0250-40df-b6d9-30a752498c86)


![image](https://github.com/user-attachments/assets/1f2080f8-9f7a-4479-b753-01815b7a3600)


